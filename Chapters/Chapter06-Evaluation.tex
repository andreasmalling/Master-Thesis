\chapter{Evaluation}
\label{cha:evaluation}
This chapter evaluates the results of experiments based on the implementation described in \autoref{cha:implementation}.

All referenced experiments in the following sections are outlined in a table by category. All tables can be found at the beginning of the section and in \autoref{app:exp_overviews}. Unless else is stated in the tables, a default steady state network is used for the introduction of the experiment (See \autoref{sec:experiment_env}). The default network consists of numerous microservices; \texttt{bootstrap} used for a local \ac{IPFS} network; \texttt{host} for serving the website; \texttt{network} for measuring transfer rates; \texttt{mongo} for storing collected data; \texttt{seeder} for seeding the \ac{MPD} as well as the video segments in the \ac{IPFS} network (See \autoref{sec:impl-overview} for all roles).

All experimental clients introduced to the steady state network, are introduced in a uniformly randomized way, with a default delay ranging from 0 and 60 \acp{s} before participating actively in the network.
The bandwidth rate for the clients is as standard at 20 \ac{Mbps} and with no artificial latency added on a closed \ac{IPFS} network, meaning that the only peers present are set up by the experiment.

All clients (except Leechers) use their local \ac{HTTP} gateway to send requests for retrieving the video segments via their own \ac{IPFS} daemon. This is considered the least complex and most desired usage of the system, and is therefore used as a baseline for the other experiments.

The video used in all experiments is Big Buck Bunny\footnote{\url{https://peach.blender.org/download/}}, shortened to show only the first 90 \ac{s} of playback and thereby making the experiments faster to run. The video is by default divided in a segment size of approximately 3000 \acp{ms} resulting in 30 segments of video data plus an initial container file. All segments amount to the size of 55.99 \acp{MB}, yielding an average bitrate of 4919 \ac{kbps}, with individual segment sizes ranging from below 450 \acp{kB} to as high as 4.1 \ac{MB}.

% High CPU usage
Scalabilty is limited to 10 clients due to high CPU usage, even though the experiments are run on a powerful \ac{VM} (See \autoref{sec:setup_vm}). This is done to avoid \textit{false} stalls, with thread scheduling as the cause. During testing each user used around 100 percent CPU (out of 1600 percent). 40 to 60 percent of this was due to the Chrome browser viewing the video, and 30 to 40 percent was because of the \ac{IPFS} daemon. The seeders daemon used less CPU at around 10 to 20 percent. Having a substitute to the \ac{DASH} player that only retrieved the pieces without viewing them could potentially double the number of users, but this has been omitted, since this would no longer evaluate the \ac{DASH} component, and the evaluation would be reduced to just how fast the system could retrieve the segments. This feature could be added as future work.

\section{Baseline experiments}
\label{sec:eval_baseline}
A Binger client is a user that watches a video and does not interact with the player after the video is started.

\begin{table}[!htbp]
    \myfloatalign
    \caption[Experimental Setup of Baseline]{Experimental Setup of \nameref{sec:eval_baseline}}
    \label{tab:exp_overview_baseline}
    \input{experiments/binge.tex}
\end{table}

The first experiment consists of the steady state network, which is introduced for 10 Binger clients after stabilization (\expid{B10}). It is expected to perform well with almost no stalls or failures, and segments are expected to be retrieved with low latency. Clients are expected to download about 56 \ac{MB} which is the size of the video, plus a small overhead of \ac{DHT} communication and duplicate data.

In the experiment (\expid{B10}) every client stalled exactly once (See \autoref{plot:baseline_stall_time}), which is expected when the video was initially started and the buffer was getting filled. This can therefore not be considered a failure since an initially empty buffer is unavoidable in the system. From this it is concluded the Bingers had a good \ac{QOE}, due to a session with the entire video and without interruptions for re-buffering.

\if\compileplots1
\input{data/baseline/stall_time}
\fi

In terms of the amount of data received by the clients, only a single peer downloaded about the expected amount, while every other client downloaded increasingly more. In the worst case about 7 times more (See \autoref{bar:baseline_network_hist}). This is very unexpected and undesirable, due to the expended bandwidth used on redundant data.

\if\compileplots1
\input{data/baseline/network_hist}
\fi

Since the video segments are the only files being distributed in the entire \ac{IPFS} network, the extra data downloaded must be duplicate data, which can be confirmed from the \ac{IPFS} \ac{CLI}, in which a metric for duplicated data download through the block exchange can be accessed\footnote{\texttt{\$ipfs stats bitswap}}. It is also observed that the ones downloading the least are also the clients that get to watch the video first, where the files are least distributed since initially only the Seeder is seeding the content, whereas the number of seeding clients increase with the run time of the experiment (See \autoref{plot:baseline_network_rx_bytes_time}). This indicates a correlation between the number of seeding clients and the overhead of duplicate data, which is very problematic.

As a consequence of this, the overhead seemingly increases with the number of seeders, potentially making the consumption of bandwidth by redundant traffic in the network throttle relevant and time critical exchanges of needed segments and thereby making the system unscalable due the narrowing bottleneck as a result of high availability.

\if\compileplots1
\input{data/baseline/network_rx_bytes_time}
\fi

In terms of latency in acquiring the files, it varies greatly between each segment and no pattern can be observed, but for the average latency of every client the best performing client has an average of 1053 \ac{ms} and the worst has an average of 912 \ac{ms} (See \autoref{tab:binge_latency_mean}), making it seem evenly distributed.

\if\compileplots1
\input{data/baseline/video_latency_mean_seg}
\fi

In spite of the worst performing Binger client downloaded the data 7 times, it does not seem to impact the clients \ac{UX} negatively. This might however change, as it is expected that the network could get swamped if this factor increases conjointly with a resource's seeders.

\subsection{Smaller network experiments}
The baseline experiment is also repeated with smaller amounts of Binge users in the network, respectively from 1 to 10 Bingers introduced to the default network. Those experiments are later referenced for comparison as baseline can be found in \autoref{tab:exp_overview_baseline}.

While these experiments primarily were done for comparison of future setups, they support the theory of a correlation between duplicate data and seeding clients, since same behaviour was observed, but with a smaller maximum overhead depending on the number of peers (See \autoref{bar:baseline_network_hist_b5} in \autoref{app:exp_plots} for example).

\subsection{Duplicate data}
\label{sec:eval_bug_dup}
Through the baseline experiments, a correlation between number of peers seeding a resource and the size of the duplicates received when requesting said resource. This is a reported bug on the go-ipfs github\footnote{\url{https://github.com/ipfs/go-ipfs/issues/4588}}, where several others experience same behaviour as observed in the baseline experiments.

This bug makes \ac{IPFS} ill-suited for the proposed purpose of effectively serving videos with high demand. In a typical \ac{P2P} network, resources being downloaded, distributes them and makes it more widely available to the rest of the network. On the other hand \ac{IPFS} also distributes the resource, but this currently results in a penalty of redundant data, which increases with the number of seeders of the resource in the network, thereby cancelling the positive effect of the increased throughput.

Since the \ac{UX} in spite of the the bug did not suffer, the planned experiments will still be evaluated. It is also possible that the bug will not be persistent and an improvement in performance is observed, since the baseline experiment suddenly can be seen as a worst case scenario, as it contains most well behaving and seeding clients. Despite the expectation of the setup being ideal for performance.

Due to this bug no experiments, where the amount of seeders are scaled, are performed, as this would simply increase the amount of duplicates the users have to receive from, resulting in worse performance.

\subsection{Seeder Download}
\label{sec:eval_seeder_rx}
In the baseline experiment (\expid{B10}) the Seeder client downloads over 16 \ac{MB} (See \autoref{bar:baseline_network_hist}), which ideally should be close to 0 as it should not request any data. The downloaded data is communication between peers over the duration of the experiment, but the download primarily happens over the 2 minutes in which the Binge clients requests the video, yielding an average download rate of $\approx$124 \ac{kBps}. Using the built in metric tool from \ac{IPFS}' \ac{CLI}\footnote{\texttt{\$ipfs stats bw} for bandwidth and \texttt{\$ipfs stats bitswap} for block exchange}, there are no reports of the Seeder downloading any blocks of data.

Since the downloaded data reported both by the \ac{CLI} and externally from the \texttt{network} service confirms this surprisingly high estimate. The downloaded data must be communication in the \acs{DHT} considering nothing else is running in the container.


\FloatBarrier \section{Leecher population experiments}
\label{sec:eval_leecher}
The Leecher acts just like a Binge user, but rather than using their own \ac{IPFS} gateway, they retrieve segments directly from the gateway of the seeder.

\begin{table}[!htbp]
\myfloatalign
\caption[Experimental Setup of Leecher]{Experimental Setup of \nameref{sec:eval_leecher}}
\label{tab:exp_overview_leecher}
\input{experiments/leecher.tex}
\end{table}

In this experiment we alter the percentage of Leechers versus Bingers that the network contains, otherwise the conditions are the same as the baseline experiment (See \autoref{tab:exp_overview_leecher} for overview).
Based on the first experiment adding Leechers to the network should decrease the amount of duplicate data being sent, but it also means more requests to the holders of the video segments potentially as there are fewer sources to get them from. Therefore the network is expected to perform better as long as the Seeder can keep up.

The results however were different than expected. Having a single Leecher in the network, had no real impact on the Binge clients, but the Leecher clients had very high stall time (See \autoref{bar:leecher_stall_time_hist}), with over 17 \ac{s} stalls spread across the video rather than just at the beginning (See \autoref{plot:leecher_stall_time}). 

%stall_time and stall_time_hist (/home/kgoyo/cs_project/home/auuser/flixtube/data/plot/2018-05-15_16:21:32.688486)
\if\compileplots1
\input{data/leecher/stall_time_leecher.tex}

\input{data/leecher/stall_time_hist_leecher.tex}
\fi

Scaling the network to having half of the users being Leechers and the other half Bingers (\expid{L5B5}), negatively impacted all the clients, as the 5 Bingers now stalled multiple times (See \autoref{bar:leecher_stall_hist}), and had an average stall time of about 13 \ac{s} while the stall time for the Leechers became much worse with an average of about 53 \ac{s}. For both types of clients the standard deviation of stall time was around 10 \ac{s} (See \autoref{bar:leecher_stall_hist_role}), meaning the users got vastly different performances, and even two of the Bingers only stalled once at the beginning of the video, meaning they got a good quality of experience.

%stall_hist, stall_hist_role_mean and stall_hist_role_stdev (/home/kgoyo/cs_project/home/auuser/flixtube/data/plot/2018-05-15_15:51:32.232356)
\if\compileplots1
\input{data/leecher/stall_hist_leecher.tex}
\input{data/leecher/stall_hist_role_leecher.tex}
\fi
%stall_time and stall_time_hist_role_mean (/home/kgoyo/cs_project/home/auuser/flixtube/data/plot/2018-05-15_16:41:32.810778)
% added in data/leecher/stall_hist_role_leecher.tex and the stall_time_hist below instead of stall_time
%\input{data/leecher/stall_time_hist_leecher10.tex}

Scaling the amount of Leechers further, to all the clients being Leechers, increased the bad quality of experience further, and the total mean stall time became 100 \ac{s}.

These results show that leeching off of a public \ac{IPFS} gateway is punished compared to the users retrieving the files themselves and using their own gateway and that the tit-for-tat principle is enforced. Unfortunately Leechers still impacted the well behaving Binge users's quality of experience as they experienced more stalling as the percentage of Leechers in the network grew.

In terms of data downloaded Leecher users were much more efficient, in the half Binger half Leecher experiment. The Leechers only downloaded on average of about 60 \ac{MB} which is only slightly higher than the file size of the video files. While the Bingers on average downloaded about 140 \ac{MB}, which is more than double of what the Leechers did (See \autoref{bar:leecher_network_hist_comparison}). 

\if\compileplots1
\input{data/leecher/network_hist_comparison.tex}
\fi

Having a network of only 5 Binger users in comparison gave an average of about 120 \ac{MB} of downloaded data, as both cases had fairly high standard deviation this is within the margin of error. But adding Leechers to the network does not seem to harm the Binger users in terms of the amount of data they have to download but instead it might lower it as they get less access to the Seeder, meaning they have to rely on the other users more.

\FloatBarrier \section{Skipper population experiments}
\label{sec:eval_skipper}
This experiment has another client behaviour in the Skipper Persona.
The Skipper Persona user acts just like the Binge user, but they interact with the video player while the video is playing. They repeat the following behaviour, watch the video for a set amount of time (referred to as watch time), and then skip forward in the video a set amount of time (referred to as skip time).

\begin{table}[!htbp]
\myfloatalign
\caption[Experimental Setup of Skipper]{Experimental Setup of \nameref{sec:eval_skipper}}
\label{tab:exp_overview_skipper}
\input{experiments/skipper.tex}
\end{table}


The goal is to see what kind of impact the Skipper has on the network as a whole and if it can negatively impact a Binge user. We also want to see what kind of performance a Skipper gets when watching a video, the video should stall every time the client skips in the video. But the amount of time spent in the stalling state should ideally be small, but based on the latency seen in the baseline experiment, it is likely that it stalls for more than a second after every skip as this is the latency seen when acquiring segments. The configuration of the Skipper is to have a watch time of 3 \ac{s} and a skip time of 10 \ac{s}. If the only stalls are because of skips, they should have exactly 7 stalls each.
In the experiments Skipper users had around 7 stalls (See \autoref{bar:skipper_stall_mean}), meaning they did not stall more than they were required to do by the Persona behaviour, the amount of skippers in the network did not impact the amount of stalls. The time spent stalling is also unaffected as can be seen from the experiment with 5 Skippers versus the one with 10. The average time spent stalling for Skipper Personas were about the same.

\if\compileplots1
\input{data/skipper/stall_hist_role_mean.tex}
%stall_hist_role_mean (/home/kgoyo/cs_project/home/auuser/flixtube/data/plot/2018-05-15_13:21:29.634478) (/home/kgoyo/cs_project/home/auuser/flixtube/data/plot/2018-05-15_13:41:31.386109)
\fi

The quality of experience for the Binge users also seemed unaffected, as they like the baseline experiment still only stalled at the start, and infrequently once more after that (See \autoref{bar:skipper_stall_hist}). 

\if\compileplots1
\input{data/skipper/stall_hist.tex}
\fi
%stall_hist (/home/kgoyo/cs_project/home/auuser/flixtube/data/plot/2018-05-15_13:21:29.634478) (/home/kgoyo/cs_project/home/auuser/flixtube/data/plot/2018-05-15_12:51:31.043665)


Although the Binge users did tend to download more data than the Skippers (See \autoref{bar:skipper_bytes_hist}), this can be excused to the Skippers going ahead in progress in the video faster than the Binge users. Meaning they will have the segments before and as the Binge users reach that point in the video and requests segments relevant to it. Those segments are more duplicated in the network resulting in downloading more duplicates.

\if\compileplots1
\input{data/skipper/bytes_hist.tex}
\fi
%rx_bytes_tx_bytes_hist_role_mean (/home/kgoyo/cs_project/home/auuser/flixtube/data/plot/2018-05-15_13:21:29.634478)


In conclusion Skippers have very little impact in relation to the network and the worsened performance is isolated within themselves with little to no negative impact on the other users in the network.

\subsection{Skipper configurations}
The Skipper can be configured in both the amount of time it watches videos in between skips, and how far forward it skips, which is tested in a single network configuration in this experiment.

The watch time is expected to have little impact as the dash.js player buffer should be about full at all times. But adjusting Skipper length means that potentially more of the buffer has to be replaced and if the Skipper length is set high enough, the entirety of the buffer is replaced. %todo find what is dash player buffer length

Increasing the skip length and decreasing watch time (\expid{S5B5-c}) had very little impact. Bingers still only stalled around a single time, and skippers still only stalled when they were performing the skip. The only notable change is that a Skipper managed to only download 36 \ac{MB} (See 2:SKIPPER in \autoref{bar:skipper_bytes_hist_configuration}) when watching for 1 second and skipping 25. Meaning they skipped enough to avoid downloading some segments, which also means that the previous buffer was unusable after the skip. 

\if\compileplots1
\input{data/skipper/byte_hist_configuration.tex}
\fi
 
\FloatBarrier \section{Incognito population experiments}
\label{sec:eval_incognito}
This experiment has the Incognito behaviour in the network.
The Incognito Persona user acts just like the Binger, but it immediately skips to the point in the video that is 10 \ac{s} before the end, and continues watching normally from there.

\begin{table}[!htbp]
    \myfloatalign
    \caption[Experimental Setup of Incognito]{Experimental Setup of \nameref{sec:eval_incognito}}
    \label{tab:exp_overview_incognito}
    \input{experiments/incognito.tex}
\end{table}

This experiment is expected to only impact a small amount of segments of the video, which are the segments that are watched by the Incognito users.
For the rest of the segments we should see similar results to the baseline experiment with equal amount of Binge users. Based on the experiments with the Skipper user, the Binge users are not expected to be affected by the Incognito user in terms of quality of experience.

In the experiment Incognito users performed like Bingers that just watched a shorter video. They stalled exactly once like Bingers, and users that watched segments late had to download more like Binge users (See \autoref{bar:incognito_network_hist}). 

\if\compileplots1
\input{data/incognito/network_hist.tex}
\fi

Unlike the Binger users the initial stall was much longer, in the 5 Binge users and 5 Incognito users experiment. Bingers stalled for an average of about 0.4 \ac{s} while Incognito was at 2 \ac{s} (See \autoref{bar:incognito_stall_role}). This delay might be because of the Incognito user initially downloads the first segments of the video and then skips to the end of the video and starts downloading new ones. The dash.js player also has to determine which segments correspond to the specific timestamp, which can add to the stalling time.

\if\compileplots1
\input{data/incognito/stall_hist_role.tex}
\fi

In terms of downloaded data the Binger users had to download slightly more compared to the baseline experiments, for instance 5 Binges with 5 Incognito users meant that the Bingers downloaded on average 145 \ac{MB}, compared to having 5 Bingers alone downloading 84 \ac{MB} (See \autoref{bar:incognito_hist_net}). This can be explained by the later segments of the video being more available, leading to an increase in duplicate data, which is in line with the other results.

\if\compileplots1
\input{data/incognito/network_rx_bytes_tx_bytes_hist_role_mean.tex}
\fi

% \FloatBarrier \section{Idle population experiments} %todo
% In this experiment we measure how adding a few idle users to the network affects Bingers.
% Idle Persona users do have an \ac{IPFS} daemon running, but do not watch any video or try to retrieve any data. But rather just fill the network with more users.
% this experiment functions as a smaller scale global \ac{IPFS} as the irrelevant peers in that network functions somewhat like the idle users in this test. It is expected that having idle users fill up the \ac{DHT}s on the Binge user should increase the amount of latency in the network slightly. But otherwise it should stay about the same.

\FloatBarrier \section{Mobile impact experiments}
\label{sec:eval_mobile}
This experiment in terms of behaviour is the same as the baseline, but the network conditions are worsened  in terms of the bandwidth available and/or latency.

\begin{table}[!htbp]
    \myfloatalign
    \caption[Experimental Setup of Mobile]{Experimental Setup of \nameref{sec:eval_mobile}}
    \label{tab:exp_overview_mobile}
    \input{experiments/mobile.tex}
\end{table}

\subsection{Low bandwidth network}
\label{sec:eval_low_bandwidth}
In these experiments the amount of bandwidth available to the clients is decreased, which should punish \ac{IPFS} more for its excessive amount of data being transmitted between peers.

By applying \autoref{eq:bandwidth} on a network with 1 Seeder and 10 Bingers, sharing a video with the bitrate of $\sim$ 5 \ac{Mbps}, the calculated lowest possible bandwidth to avoid stalls would be $\approx$ 9.5 \ac{Mbps}.

Decreasing the bandwidth below the videos average bitrate, would make it impossible for a client to watch the video without stalls, even in an ideal configuration with a direct connection to a host with no other network communication present.

In the experiment (\expid{B10-m1}) lowering the bandwidth to 10 \ac{Mbps} was enough to achieve a very poor viewing experience. Each user got 9 or more stalls throughout the video (see \autoref{bar:mobile_bandwidth_stall_hist}), with a mean accumulated stall time of 77 \ac{s}, which approximates the duration of the desired video.

This high amount of stalling is likely due to the large amount of duplicate data in the network (see \autoref{sec:eval_bug_dup}), as the scarce bandwidth is wasted on getting duplicate segments rather than retrieving future segments relevant to the user. This results in the \ac{DASH} buffer running out of segments due to \ac{IPFS} downloading the same segment multiple times.

Due to the performance of this experiment, there was no need to perform further experimentation with lower download rates.

\if\compileplots1
\input{data/mobile/bandwidth/stall_hist.tex}
\fi

\subsection{High latency network}
\label{sec:eval_high_latency}
High latency is expected to increase the amount of duplicate data the clients receive, as changes in the peers want list take longer to propagate to peers that have segments that are no longer wanted. It could potentially also increase the amount of stalls, since it takes too long for the relevant segments to arrive to the client that needs them. An added latency of 100 \ac{ms} is chosen (based on a ping to Google's data center in Sydney, Australia).
In the experiments the increased latency significantly reduced the quality of experience, as half of the users now had an extra stall (see \autoref{bar:mobile_latency_stall_hist}), which were about halfway into the video and lasting multiple seconds (see \autoref{plot:mobile_latency_stall_time}). This is in line with the hypothesis that it takes too long to get relevant pieces.

\if\compileplots1
\input{data/mobile/latency/stall_hist.tex}
\input{data/mobile/latency/stall_time.tex}
\fi

But the amount of duplicate data did not increase as much as expected as the average amount of received bandwidth only increased by 12 \ac{MB} (see \autoref{bar:mobile_latency_bytes_hist_role}). The standard deviation also decreased from increasing the latency, meaning the amount of downloaded data for each user was more consistent with each other. And there are less users that have to download significantly more. This however comes at the cost of each user having to download more on average.

\if\compileplots1
\input{data/mobile/latency/bytes_hist_role.tex}
\fi

\subsection{Jittery network conditions}
In this experiment the conditions of mobile users were to be emulated, where the connection fluactuates. The conditions should vary between a good connection (high bandwidth such as 4G) and low bandwidth or no connection at all for short amounts of time.
The results of the experiment were expected to be a good \ac{UX} as long as the conditions are not bad for too long, due to the buffer being filled which should be able to keep the video running smoothly.

This experiment was not conducted, though highly relevant, due to limitations in the pumba tool, which only enabled jitter for latency and not bandwidth rate.
This could have been possible given more time, \eg by modifying the run script to accept series of individual bandwidth rates to enforce sequentially upon each particular mobile client.

\FloatBarrier \section{Leaver impact experiments}
\label{sec:eval_leaver}
In this experiment the clients  leave the \ac{IPFS} network after they have finished watching the video, meaning they would no longer provide the video to other peers, when they themselves have finished.

\begin{table}[!htbp]
    \myfloatalign
    \caption[Experimental Setup of Leaver]{Experimental Setup of \nameref{sec:eval_leaver}}
    \label{tab:exp_overview_leaver}
    \input{experiments/leaver.tex}
\end{table}

The experiment is based on the baseline (\expid{B10}), but where clients leave when they are done. Based on the results of the baseline this should result in reduced duplicate data as the clients that join the network early, leave before the remaining clients are finished. And when they do that is one less duplicate that the remaining clients receive. In terms of quality of experience there should be no major difference.
The experiment went very much like expected, the amount of stalls was the same as the baseline, but the mean amount of downloaded data was decreased by 17 \ac{MB} (See \autoref{bar:leaver_bytes_hist}). The difference in received data between the peers also lowered drastically as can be seen by the almost halved standard deviation. This is likely due to fewer sources for retrieving data being available to the users that play the video later, meaning these users receive less duplicates.

\if\compileplots1
\input{data/leaver/bytes_hist.tex}
\fi

\FloatBarrier \section{Socialness impact experiments}
\label{sec:eval_socialness}
The other experiments focused on having all clients watch the video at the same time within a 60 \ac{s} startup interval.

\begin{table}[!htbp]
    \myfloatalign
    \caption[Experimental Setup of Socialness]{Experimental Setup of \nameref{sec:eval_socialness}}
    \label{tab:exp_overview_socialness}
    \input{experiments/socialness.tex}
\end{table}

This experiment changes the time between startup effectively changing the socialness of the video. \expid{B10-s1} lowers the startup delay to 10 \ac{s}, effectively increasing the socialness. \expid{B10-s2} on the other hand lowers the socialness as the startup delay is 180 \ac{s}.

Although to properly test the impact of socialness the experiments have to last much longer, multiple days, which is omitted and replaced with the smaller time frame experiment instead.

In the experiments increasing the maximum startup delay gave a universally better performance. With the longer startup delay from \expid{B10-s2} there were only the initial stalls just like the baseline, but lowering it, as done in \expid{B10-s2}, meant stalling became frequent. (See \autoref{bar:social_stall_hist}) The high amount of stalling is likely due to pressure on the Seeder as more users now want segments only available at the Seeder, as they have not been distributed yet.

Increasing the startup delay, also decreases the amount of duplicate data the users receive (See \autoref{bar:social_bytes_hist}) which is unusual as adding more users with the data available, usually meant more duplicate data. %todo this is odd 

\if\compileplots1
\input{data/social/stall_hist.tex}
\input{data/social/bytes_hist.tex}
\fi

\FloatBarrier 
\section{Video content influence}
\label{sec:eval_video}
In these experiments the video files are altered, to observe which influence the content being shared have on the network.

\begin{table}[!htbp]
    \myfloatalign
    \caption[Experimental Setup of Segment size]{Experimental Setup of \nameref{sec:eval_video}}
    \label{tab:exp_overview_video}
    \input{experiments/video.tex}
\end{table}

\subsection{Increased segment duration experiments}
\label{sec:eval_segment}

In this experiment each segment is around 10 \ac{s} long rather than the default 3 \ac{s}. This is to try to capitalize on the way \ac{IPFS} separates files into blocks itself rather than doing it manually, as having many small segments means that each segment is only a few blocks. This can however impact the user experience, as users now have to download a larger segment before they can play anything from that segment's time frame.

In the experiments all the clients stalled exactly twice, although the second of these stalls can not be considered bad \ac{UX} because it happens on average 30 \ac{ms} after the video starts. A frame lasts 33 \ac{ms} for the video used in the experiment, so this stall only lasts around a single frame. The length of the second stall is also very small at an average 32 \ac{ms}, which should not be noticeable when it happens a single frame into the video. The time spent stalling is however very significant. In the experiment the initial stall lasted 6.9 \ac{s} longer than the baseline (see \autoref{bar:segment_stall_time_hist}). In terms of downloaded data the experiment performed better than the baseline The mean amount of received bandwidth was about two thirds of the baseline, with much lower standard deviation too signifying that \ac{IPFS} is more efficient at handling larger files where it can utilize the way the files are split into blocks in the Bitswap. This however comes at the cost of the initial stall time.

\if\compileplots1
\input{data/segment/stall_time_hist.tex}
\input{data/segment/bytes_hist.tex}
\fi

\subsection{Increased bitrate experiment}
In this experiment the bitrate of the video is increased so that the total file size is 117\ac{MB}, which is approximately double of the video file size in the baseline experiment. To accommodate this increase in bitrate the network bandwidth is also doubled to prevent the same amount of stalls as seen in \expid{B10-m1}. Based on \expid{B10-v9} the amount of duplicate data relative to the filesize should decrease, as \ac{IPFS} is expected to perform better on larger files.
In the experiment a few of the users stalled a single time in the middle of the video as the only \ac{UX} penalty. In terms of duplicate data increasing the bitrate resulted in less as expected. (See \autoref{bar:bitrate_bitrate}) In the baseline the amount of data was 3.6 times the file size, but with increased bitrate this fell to 3.2 times. This further cements the theory that \ac{IPFS} is better suited for larger files rather than small ones.

\if\compileplots1
\input{data/bitrate/bitrate.tex}
\fi

\FloatBarrier \section{Global IPFS experiments}
\label{sec:eval_global}

In the other experiments all clients in the \ac{IPFS} network were relevant to the video files, contrary to the intended design of \ac{IPFS} where all users of \ac{IPFS} are in a shared network regardless of the content they have.

\begin{table}[!htbp]
    \myfloatalign
    \caption[Experimental Setup of Global \acs{IPFS}]{Experimental Setup of \nameref{sec:eval_global}}
    \label{tab:exp_overview_global}
    \input{experiments/global.tex}
\end{table}

This experiment runs the same configuration as the baseline but operates in the global \ac{IPFS} instead, meaning it is using the default bootstrap addresses to connect to \ac{IPFS} network.
It is expected to perform similar to the high latency experiment, as having larger network means larger hash tables, which results in having longer look up times for the video segments, due to the \ac{DHT}'s worst case lookup time being $ O ( \log n)$, where $n$ is the number of peers in the network \cite[p.2]{benet2014ipfs}.


During the experiments some clients experienced multiple stalls (see \autoref{bar:global_stall_hist}). These stalls where also fairly lengthy, up to 7 \ac{s}, and around the middle of the video (see \autoref{plot:global_stall_time}). Over half of the population suffered from additional stalls and encountered a longer re-buffering event during playback than the initial buffering, which signifies a poor \ac{UX} of the viewing experience.

\if\compileplots1
\input{data/global/stall_hist.tex}
\input{data/global/stall_time.tex}
\fi

In terms of bandwidth there were slight improvements compared to the baseline experiment (\expid{B10}) in which 3 users had around 300 \ac{MB} or more received data; this overhead has now evened out between the clients, since the worst case of received data is around 250 \ac{MB} (see \autoref{bar:global_bytes_hist} compared to \autoref{bar:baseline_network_hist}).

The mean of the accumulated received data however did not improve and was around the same for the experiments compared to the baseline (see \autoref{bar:global_bytes_hist_role}), but the standard deviation decreased as the overhead was more evenly distributed among the clients.
These results were very similar to that of the increased latency experiment as expected. Just like that experiment users get slightly more stalls, and the mean received data increased and the standard deviation decreased.

\if\compileplots1
\input{data/global/bytes_hist.tex}
\input{data/global/bytes_hist_role.tex}
\fi

% stall_time        stall_hist         network

\FloatBarrier 
\section{Summary of Evaluation}
\ac{IPFS} has several shortcomings when it comes to performance, as discovered in this evaluation.

The first of which is its large \ac{CPU} usage, which significantly limits how many peers can be simulated, and thereby the scalability of the experiments.
This issue is further worsened by the high \ac{CPU} usage of the \ac{DASH} video player, which potentially could be lowered using another browser or other \acp{OS} as Linux does not have hardware accelerated video decoding available for Chrome\footnote{\url{https://bugs.chromium.org/p/chromium/issues/detail?id=137247}}. Since the current setup utilizes Docker for emulation, a change in \ac{OS} would be cumbersome as Docker only emulates Linux. However a change in browser to Firefox\footnote{\url{https://www.mozilla.org/firefox/}} would be very doable as Splinter also has a driver for this browser.

Despite the small network setup of maximum 12 \ac{IPFS} peers (bootstrap, Seeder and up to 10 clients), another scaling issue of \ac{IPFS} was apparent. As the number of peers seeding a file grew, the amount of duplicate data did as well. This transfer overhead meant that lowering the bandwidth to 10 \ac{Mbps} resulted in very poor \ac{QOE}, despite it theoretically being enough. Combine this with the fact that adding latency or using the global \ac{IPFS} network, further increases the amount of duplicate data, and it is clear that \ac{IPFS} is ill-suited for mobile devices as they are subject to these worsened network conditions, and often on a limited data plan making data overhead even more costly. 

There are however ways to lessen the amount of duplicate data a user receives.
The first is to have users only share their file segments while they are still streaming the video.
with this strategy less peers seed the file segments at any given time, diminishing the connections serving the duplicated data. This strategy is however dangerous as the segments corresponding to the later parts of the video, are not stored for very long on a peer before it leaves the network, which means these segments could potentially disappear from the network if there were no one left to seed them. 

Another way to reduce the data duplication was to give the users a larger time span between when they started the video. The cause of this is unknown, but a possible explanation could be that the BitSwap of \ac{IPFS}, the protocol used for block exchange, puts more effort in distributing files that a peer recently received. This would be due to the less time there is between the introduced clients, the more wanted the same segments will be as the playback is contiguous.
This phenomenon makes \ac{IPFS} fairly similar to a centralized system as it does not handle sudden large influx in users, similar to a \ac{DDoS} attack.

The final way to reduce duplication of data was to increase the segment duration of the dashed video files. This comes at the cost of increasing the initial wait time before the video can start playing, due to the segment size being larger than what was necessary to begin playback. This behaviour indicates that \ac{IPFS} is more suited for retrieving few large files rather than many small ones, which is further backed up by the fact that increasing bitrate also decreased the ratio of duplicated data compared to required data.

In terms of user behaviours the negative experiences caused by these were mostly isolated within the \ac{UX} of the client causing these. Leeching resulted in more frequent stalling of the Leechers playback, while the other users could continue normally. This was until the population of Leechers was so large that the Seeder received too many requests to accommodate all. In terms of viewing patterns, skipping in the video resulted in stalls during the skip, which was expected behaviour as playback would require segments outside the buffer, but this had no negative impact on \ac{UX} of the other clients in the network.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../ClassicThesis"
%%% ispell-dictionary: "british" ***
%%% mode:flyspell ***
%%% mode:auto-fill ***
%%% fill-column: 76 ***
%%% End:
